{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Introduction to Pytorch</font>\n",
    "This notebook will provide a brief overview of PyTorch and how it is similar to Numpy. The goal of this notebook is to understand the basic data structures required to build Deep Learning models and train them.\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Why do we need PyTorch when we already have Numpy?</font>\n",
    "Deep Learning involves performing similar operations like convolutions and multiplications repetitively. Thus there is a need to run the code on GPUs which can parallelize these operations over multiple cores - these devices are perfectly suitable for doing massive matrix operations and are much faster than CPUs.\n",
    "\n",
    "While NumPy with its various backends suits perfectly for doing calculus on CPU, it lacks the GPU computations support. And this is the first reason why we need Pytorch.\n",
    "The other reason is that Numpy is a genral purpose library. PyTorch ( or any other modern deep learning library ) has optimized code for many deep learning specific operations (e.g. Gradient calculations ) which are not present in Numpy.\n",
    "So, let's take a look at what are the data structures of Pytorch and how it provides us its cool features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Tensor - Pytorch's core data structure</font>\n",
    "\n",
    "As we've already seen, in python we can create lists, lists of lists, lists of lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`-dimensional array, e.g. 3-dimensional if we want to find an analog for a list of lists of lists. In math, there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor.\n",
    "\n",
    "Tensor is an entity with a defined number of dimensions called an order (rank).\n",
    "\n",
    "**Scalars** can be considered as a rank-0-tensor. Let's denote scalar value as $x \\in \\mathbb{R}$, where $\\mathbb{R}$ is a set of real numbers.\n",
    "\n",
    "**Vectors** can be introduced as a rank-1-tensor. Vectors belong to linear space (vector space), which is, in simple terms, a set of possible vectors of a specific length. A vector consisting of real-valued scalars ($x \\in \\mathbb{R}$) can be defined as ($y \\in \\mathbb{R}^n$), where $y$ - vector value and $\\mathbb{R}^n$ - $n$-dimensional real-number vector space. $y_i$ - $i_{th}$ vector element (scalar):\n",
    "\n",
    "$$\n",
    " y = \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Matrices** can be considered as a rank-2-tensor. A matrix of size $m \\times n$, where $m, n \\in \\mathbb{N}$ (rows and columns number accordingly) consisting of real-valued scalars can be denoted as $A \\in \\mathbb{R}^{m \\times n}$, where $\\mathbb{R}^{m \\times n}$ is a real-valued $m \\times n$-dimensional vector space:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "    x_{11}       & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
    "    x_{21}       & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1}       & x_{m2} & x_{m3} & \\dots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the picture below you can find different tensor dimensions, where the fourth and the fifth cases are a vector of cubes and a matrix of cubes accordingly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<table style=\"border: 5px solid red\">\n",
    "    <tr><th><center>Rank-1-tensor</center></th><th><center>Rank-2-tensor</center></th><th><center>Rank-3-tensor</center></th><th><center>Rank-4-tensor</center></th><th><center>Rank-5-tensor</center></th></tr>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_1D_tensor_.jpg\" width=\"30\" height=\"30\"></center></th>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_2D_tensor.jpg\" width=\"130\" height=\"130\"></center></th>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_3D_tensor.jpg\" width=\"145\" height=\"145\"></center></th>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_4D_tensor_.jpg\" width=\"120\" height=\"120\"></center></th>\n",
    "        <th><center><img align=\"middle\" src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w1_matrix_of_cubes.jpg\" width=\"240\" height=\"240\"></center></th>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Tensor basics</font>\n",
    "Let's import the torch module first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Tensor Creation </font>\n",
    "Let's view examples of matrices and tensors generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2-dimensional (rank-2) tensor of zeros:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Random rank-4 tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0951, 0.1100],\n",
       "          [0.1467, 0.2955]],\n",
       "\n",
       "         [[0.3357, 0.5836],\n",
       "          [0.5688, 0.4980]]],\n",
       "\n",
       "\n",
       "        [[[0.3024, 0.3346],\n",
       "          [0.0965, 0.0114]],\n",
       "\n",
       "         [[0.4109, 0.3194],\n",
       "          [0.8906, 0.6340]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Python / NumPy / Pytorch interoperability</font>\n",
    "You can create tensors from python lists as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List:    [1, 2]\n",
      "Array:   [1 2]\n",
      "Tensor:  tensor([1, 2])\n",
      "Tensor:  tensor([1, 2])\n",
      "Tensor:  tensor([1, 2])\n",
      "Array:   [1 2]\n"
     ]
    }
   ],
   "source": [
    "# Simple Python list\n",
    "python_list = [1, 2]\n",
    "\n",
    "# Create a numpy array from python list\n",
    "numpy_array = np.array(python_list)\n",
    "\n",
    "# Create a torch Tensor from python list\n",
    "tensor_from_list = torch.tensor(python_list)\n",
    "\n",
    "# Create a torch Tensor from Numpy array\n",
    "tensor_from_array = torch.tensor(numpy_array)\n",
    "\n",
    "# Another way to create a torch Tensor from Numpy array (Share same storage)\n",
    "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
    "\n",
    "# Convert torch tensor to numpy array\n",
    "array_from_tensor = tensor_from_array.numpy()\n",
    "\n",
    "print('List:   ', python_list)\n",
    "print('Array:  ', numpy_array)\n",
    "print('Tensor: ', tensor_from_list)\n",
    "print('Tensor: ', tensor_from_array)\n",
    "print('Tensor: ', tensor_from_array_v2)\n",
    "print('Array:  ', array_from_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Differnce between `torch.Tensor` and `torch.from_numpy`\n",
    "\n",
    "Pytorch aims to be an effective library for computations. What does it mean? It means that pytorch avoids memory copying if it can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array:   [10  2]\n",
      "Tensor:  tensor([1, 2])\n",
      "Tensor:  tensor([10,  2])\n"
     ]
    }
   ],
   "source": [
    "numpy_array[0] = 10\n",
    "\n",
    "print('Array:  ', numpy_array)\n",
    "print('Tensor: ', tensor_from_array)\n",
    "print('Tensor: ', tensor_from_array_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, we have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It also works in the opposite way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:  tensor([1, 2])\n",
      "Array:  [1 2]\n",
      "Tensor:  tensor([11,  2])\n",
      "Array:  [11  2]\n"
     ]
    }
   ],
   "source": [
    "array_from_tensor = tensor_from_array.numpy()\n",
    "print('Tensor: ', tensor_from_array)\n",
    "print('Array: ', array_from_tensor)\n",
    "\n",
    "tensor_from_array[0] = 11\n",
    "print('Tensor: ', tensor_from_array)\n",
    "print('Array: ', array_from_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Data types</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The basic data type for all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch supports different numbers types for its tensors the same way as NumPy does it - by specifying the data type on tensor creation or via casting. The full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor with default type:  tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "Tensor with 16-bit float:  tensor([[0., 0.],\n",
      "        [0., 0.]], dtype=torch.float16)\n",
      "Tensor with integers:  tensor([[0, 0],\n",
      "        [0, 0]], dtype=torch.int16)\n",
      "Tensor with boolean data:  tensor([[False, False],\n",
      "        [False, False]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros(2, 2)\n",
    "print('Tensor with default type: ', tensor)\n",
    "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
    "print('Tensor with 16-bit float: ', tensor)\n",
    "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
    "print('Tensor with integers: ', tensor)\n",
    "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
    "print('Tensor with boolean data: ', tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Indexing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special functions calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Joining a list of tensors together with `torch.cat`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(3, 2)\n",
    "b = torch.ones(3, 2)\n",
    "print(torch.cat((a, b), dim=0))\n",
    "print(torch.cat((a, b), dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Indexing with another tensor/array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "[False False False False False False  True  True  True  True]\n",
      "tensor([6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
      "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(start=0, end=10)\n",
    "indices = np.arange(0, 10) > 5\n",
    "print(a)\n",
    "print(indices)\n",
    "print(a[indices])\n",
    "\n",
    "indices = torch.arange(start=0, end=10) % 5\n",
    "print(indices)\n",
    "print(a[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Tensor Shapes</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`.\n",
    "\n",
    "The difference is the following:\n",
    "- view tries to return the tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reasons](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails.\n",
    "- reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy.\n",
    "\n",
    "Let's see with the help of an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointer to data:  140221062150848\n",
      "Shape:  torch.Size([2, 3, 4])\n",
      "Reshaped tensor - pointer to data 140221062150848\n",
      "Reshaped tensor shape  torch.Size([24])\n",
      "Viewed tensor - pointer to data 140221062150848\n",
      "Viewed tensor shape  torch.Size([3, 2, 4])\n",
      "Original stride:  (12, 4, 1)\n",
      "Reshaped stride:  (1,)\n",
      "Viewed stride:  (8, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(2, 3, 4)\n",
    "print('Pointer to data: ', tensor.data_ptr())\n",
    "print('Shape: ', tensor.shape)\n",
    "\n",
    "reshaped = tensor.reshape(24)\n",
    "\n",
    "view = tensor.view(3, 2, 4)\n",
    "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
    "print('Reshaped tensor shape ', reshaped.shape)\n",
    "\n",
    "print('Viewed tensor - pointer to data', view.data_ptr())\n",
    "print('Viewed tensor shape ', view.shape)\n",
    "\n",
    "assert tensor.data_ptr() == view.data_ptr()\n",
    "\n",
    "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
    "\n",
    "print('Original stride: ', tensor.stride())\n",
    "print('Reshaped stride: ', reshaped.stride())\n",
    "print('Viewed stride: ', view.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "torch.Size([3, 2, 4])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(tensor.reshape(3, 2, 4).shape)\n",
    "print(tensor.reshape(3, 2, -1).shape)\n",
    "print(tensor.reshape(3, -1, 4).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Autograd</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pytorch supports automatic differentiation. The module which implements this is called **Autograd**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `requires_grad` flag. But, for advanced tensors, it is enabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7106, 0.0810, 0.8641, 0.8580, 0.6997],\n",
      "        [0.3955, 0.2821, 0.6382, 0.5523, 0.6642],\n",
      "        [0.7849, 0.5390, 0.5036, 0.3130, 0.4855]], requires_grad=True)\n",
      "tensor([[3.5530, 0.4050, 4.3204, 4.2898, 3.4984],\n",
      "        [1.9777, 1.4107, 3.1908, 2.7614, 3.3211],\n",
      "        [3.9246, 2.6951, 2.5179, 1.5651, 2.4274]], grad_fn=<MulBackward0>)\n",
      "tensor([[5., 5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3, 5), requires_grad=True)\n",
    "print(a)\n",
    "result = a * 5\n",
    "print(result)\n",
    "\n",
    "# grad can be implicitly created only for scalar outputs\n",
    "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
    "mean_result = result.sum()\n",
    "# Calculate Gradient\n",
    "mean_result.backward()\n",
    "# Print gradient of a\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Disabling Autograd for tensors </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
    "\n",
    "1. `detach` - returns a copy of the tensor with autograd disabled. This copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) modifications are **not** allowed.\n",
    "1. `torch.no_grad()` - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 5), requires_grad=True)\n",
    "detached_a = a.detach()\n",
    "detached_result = detached_a * 5\n",
    "result = a * 10\n",
    "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
    "# so let's calculate the sum here\n",
    "mean_result = result.sum()\n",
    "mean_result.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10.],\n",
       "        [10., 10., 10., 10., 10.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((3, 5), requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    detached_result = a * 5\n",
    "result = a * 10\n",
    "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
    "# so let's calculate the sum here\n",
    "mean_result = result.sum()\n",
    "mean_result.backward()\n",
    "a.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
